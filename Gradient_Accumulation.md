### 1. æç®€ä½¿ç”¨ Gradient Accumulation

```python
accum_steps = 4
optimizer.zero_grad(set_to_none=True)

pending = 0
buf = []  # åªä¸ºçŸ¥é“å°¾ç»„å¤§å°ï¼Œä¹Ÿå¯ç”¨è®¡æ•°å™¨

for x, y in loader:              # loader çš„ batch_size = B
    buf.append(1)
    group_size = accum_steps if len(buf) < accum_steps else len(buf)

    # criterion æ˜¯æŸå¤±å‡½æ•°, model(x) æ˜¯æ¨¡å‹çš„é¢„æµ‹, y æ˜¯ label
    # group_size è¡¨ç¤ºæ€»çš„ batch size, æ€»å…±æœ‰é‚£ä¹ˆå¤šæ•°é‡çš„ sampleï¼Œæ¯ä¸€ä¸ª Loss æœ¬èº«ä¸è¦æ±‚ mean
    # è¿™ä¸ª backward() å¯¹å½“å‰å‰å‘å›¾åšåå‘ä¼ æ’­ï¼Œè®¡ç®—æ¯ä¸ªå¯è®­ç»ƒå‚æ•°çš„ âˆ‚loss/âˆ‚ğœƒï¼Œå¹¶ç´¯åŠ åˆ° param.grad é‡Œ

    (criterion(model(x), y) / group_size).backward()

    '''
        [1] æ¯æ¬¡ loss.backward() å®Œæˆåï¼Œè¿™æ¬¡å‰å‘çš„è®¡ç®—å›¾ï¼ˆactivations ç­‰ï¼‰ä¼šè¢«é‡Šæ”¾ï¼ˆé»˜è®¤ retain_graph=Falseï¼‰ã€‚
        ä¸‹ä¸€æ¬¡ micro-batch ä¼šé‡æ–°å‰å‘ã€é‡æ–°æ„å›¾ï¼Œå› æ­¤æ¿€æ´»æ˜¾å­˜ä¸ä¼šéšç€ç´¯ç§¯æ­¥æ•°çº¿æ€§å¢é•¿

        [2] param.grad å¼ é‡ä¼šåœ¨ç¬¬ä¸€æ¬¡ backward() æ—¶åˆ†é…ï¼Œæ­¤ååœ¨ç»„å†…å¤ç”¨å¹¶ç´¯åŠ ï¼ˆæ˜¾å­˜åŸºæœ¬æ’å®šï¼‰

        [3] åªæœ‰åœ¨ä½  zero_grad(set_to_none=True) æ—¶ï¼Œæ‰æŠŠ grad è®¾å› Noneï¼Œä»è€Œé‡Šæ”¾æ¢¯åº¦ç¼“å†²ï¼›å¦åˆ™è‹¥ç½®é›¶ï¼Œä¸ä¼šé‡Šæ”¾é‚£å—å†…å­˜
    '''

    pending += 1

    
    if pending == accum_steps:
        # ç”¨å½“å‰ param.grad å¯¹å‚æ•°åšä¸€æ¬¡æ›´æ–°ï¼ˆå¦‚ SGD/Adamï¼‰ï¼Œç„¶å param.data è¢«æ”¹å˜
        optimizer.step()
        # æŠŠ param.grad æ¸…ç©º (ç½® 0 æˆ–ç½® None)
        optimizer.zero_grad(set_to_none=True)
        pending = 0
        buf.clear()

# å¤„ç†æœ€åä¸è¶³ä¸€ç»„
if pending:
    optimizer.step()
    optimizer.zero_grad(set_to_none=True)
```


### 2. ä¸ä½¿ç”¨ Gradient Accumulationï¼Œç›´æ¥æ”¾å¤§ batchsize

```python
big_loader = DataLoader(dataset, batch_size=B*4, shuffle=True, drop_last=False)

optimizer.zero_grad(set_to_none=True)
for x, y in big_loader:
    loss = criterion(model(x), y)   # ä¸éœ€è¦é™¤ä»¥ 4
    loss.backward()
    optimizer.step()
    optimizer.zero_grad(set_to_none=True)
```
